{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *What is your dataset?*\n",
    "\n",
    "Our dataset is the [MTA Subway Hourly Ridership](https://data.ny.gov/Transportation/MTA-Subway-Hourly-Ridership-2020-2024/wujg-7c2s/about_data) dataset provided by the Metropolitan Transportation Authority (MTA) of New York. It includes hourly counts of entries and exits for every subway station in NYC from January 2020 to the present. Each row represents one station-hour, with metadata including date, station name, line, borough, and ridership numbers. The dataset captures both weekday and weekend traffic, making it useful for analysing temporal patterns in New York’s subway system over time.\n",
    "\n",
    "- *Why did you choose this/these particular dataset(s)?*\n",
    "\n",
    "We chose it because the subway reflects the mood, motion, and disruption of the city. Whether it is a blizzard in The Bronx, a signal failure at Penn Station, or 50,000 runners flooding Columbus Circle, the changes in ridership give us a direct, measurable signal of how New Yorkers react to their environment.\n",
    "\n",
    "And because the dataset is so granular—hour-by-hour at each station—it allowed us to zoom in on the exact moment when the city shifts. We could see the crowd surge in Times Square at midnight, the quiet after a protest, or the hesitation after a high-profile crime.\n",
    "\n",
    "- *What was your goal for the end user's experience?*\n",
    "\n",
    "Our aim was to let readers feel the rhythm of the city. We did not just want to show charts—we wanted to tell stories. Stories of how the subway reacts during a crisis, how it pulses with energy during events, and how it quietly reflects collective human decisions like protest, celebration, or avoidance.\n",
    "\n",
    "We designed our story like a digital magazine: immersive, browsable, with clean visuals that flow naturally from one insight to the next. We wanted users to walk away not just understanding the data—but seeing the subway as a living, breathing mirror of New York City itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Write about your choices in data cleaning and preprocessing*.\n",
    "\n",
    " Subway data is messy—just like the city it tracks. Each entry in the dataset records a turnstile tap at a specific moment in time, logged under a column called `transit_timestamp`. But for our stories to work—like isolating midnight surges on New Year’s Eve or early-morning Marathon crowds—we needed a clearer sense of when things were happening.\n",
    "\n",
    "So, we did one crucial thing:\n",
    "\n",
    "```python\n",
    "# Convert timestamp to datetime and extract date/hour\n",
    "df['transit_timestamp'] = pd.to_datetime(df['transit_timestamp'])\n",
    "df['date'] = df['transit_timestamp'].dt.date\n",
    "df['hour'] = df['transit_timestamp'].dt.hour\n",
    "```\n",
    "\n",
    "Why?\n",
    "Because human stories do not live in timestamps—they live in hours and days. We needed to know:\n",
    "\n",
    "* What happens at 8 AM on a Monday?\n",
    "* How do late-night entries change on New Year’s Eve?\n",
    "* What’s the average 6 PM weekday crowd compared to a Saturday?\n",
    "\n",
    "Extracting date and hour helped us group the data meaningfully, so we could build visualisations like hourly ridership patterns, event overlays, and weekend vs. weekday comparisons. Without this step, all of those stories would have stayed hidden in raw strings of numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\poltr\\AppData\\Local\\Temp\\ipykernel_36096\\1957882157.py:5: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 845. MiB for an array with shape (110696365,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m file_path = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mpoltr\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mOneDrive - udl.cat\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDesktop\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mMTA_Subway_Hourly_Ridership__2020-2024_20250408.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Read data from the specified file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Display the first few rows of the dataframe\u001b[39;00m\n\u001b[32m      8\u001b[39m df.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\poltr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\poltr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\poltr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1968\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1965\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1966\u001b[39m         new_col_dict = col_dict\n\u001b[32m-> \u001b[39m\u001b[32m1968\u001b[39m     df = \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_col_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1975\u001b[39m     \u001b[38;5;28mself\u001b[39m._currow += new_rows\n\u001b[32m   1976\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\poltr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\poltr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\poltr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:119\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     arrays, refs = \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    127\u001b[39m     index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\poltr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:629\u001b[39m, in \u001b[36m_homogenize\u001b[39m\u001b[34m(data, index, dtype)\u001b[39m\n\u001b[32m    626\u001b[39m         val = \u001b[38;5;28mdict\u001b[39m(val)\n\u001b[32m    627\u001b[39m     val = lib.fast_multiget(val, oindex._values, default=np.nan)\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m val = \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m com.require_length_match(val, index)\n\u001b[32m    631\u001b[39m refs.append(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\poltr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\construction.py:606\u001b[39m, in \u001b[36msanitize_array\u001b[39m\u001b[34m(data, index, dtype, copy, allow_2d)\u001b[39m\n\u001b[32m    604\u001b[39m subarr = data\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data.dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m606\u001b[39m     subarr = \u001b[43mmaybe_infer_to_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    608\u001b[39m         object_index\n\u001b[32m    609\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m using_pyarrow_string_dtype()\n\u001b[32m    610\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m is_string_dtype(subarr)\n\u001b[32m    611\u001b[39m     ):\n\u001b[32m    612\u001b[39m         \u001b[38;5;66;03m# Avoid inference when string option is set\u001b[39;00m\n\u001b[32m    613\u001b[39m         subarr = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\poltr\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1189\u001b[39m, in \u001b[36mmaybe_infer_to_datetimelike\u001b[39m\u001b[34m(value)\u001b[39m\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[32m   1186\u001b[39m \u001b[38;5;66;03m# error: Incompatible return value type (got \"Union[ExtensionArray,\u001b[39;00m\n\u001b[32m   1187\u001b[39m \u001b[38;5;66;03m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001b[39;00m\n\u001b[32m   1188\u001b[39m \u001b[38;5;66;03m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaybe_convert_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[32m   1190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[39;49;00m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#  numpy would have done it for us.\u001b[39;49;00m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_non_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_if_all_nat\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mM8[ns]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2540\u001b[39m, in \u001b[36mpandas._libs.lib.maybe_convert_objects\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 845. MiB for an array with shape (110696365,) and data type int64"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\poltr\\OneDrive - udl.cat\\Desktop\\MTA_Subway_Hourly_Ridership__2020-2024_20250408.csv\"\n",
    "\n",
    "# Read data from the specified file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dataset size and columns\n",
    "print(f\"Dataset size: {df.shape[0]} rows and {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **This is a comment section**\n",
    "\n",
    "Once cleaned, we dove into the data like rush hour at Grand Central.\n",
    "\n",
    "* **Size**: Over 280 million entries** across **500+ unique stations**, covering **2020–2024**.\n",
    "* **Granularity**: Hourly turnstile counts, letting us track **minute-by-minute shifts** in city behaviour.\n",
    "* **Coverage**: Every borough, every station, every day—including **lockdowns**, **storms**, **holidays**, and **historic protests**.\n",
    "\n",
    "From our exploratory data analysis, some fascinating patterns emerged:\n",
    "\n",
    "* **Weekday Peaks**: Predictable morning (7–9 AM) and evening (5–7 PM) rushes.\n",
    "* **Pandemic Drop**: A sharp fall in ridership during spring 2020, bottoming out in April.\n",
    "* **Event Spikes**: Localised surges during marathons, parades, and holidays—especially at stations like **Times Sq–42 St**, **Columbus Circle**, and **Atlantic Av–Barclays Ctr**.\n",
    "\n",
    "We also created:\n",
    "\n",
    "* Ridership histograms by hour and weekday\n",
    "* Heatmaps of station-level usage on special dates\n",
    "* Bar charts showing drops tied to major disruptions (e.g., signal failures, snowstorms)\n",
    "\n",
    "This stage let us frame our story: not as a static chart, but as a moving timeline—one tap at a time.\n",
    "\n",
    "# **This is a comment section (till here)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which columns can we remove?**\n",
    "\n",
    "\n",
    "**Which rows of columns should we remove?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Write a short section that discusses the dataset stats, containing key points/plots from your exploratory data analysis.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "- *Describe your data analysis and explain what you've learned about the dataset.*\n",
    "- *If relevant, talk about your machine-learning.*\n",
    "\n",
    "##### **DATA ANALYSIS IS CARRIED OUT AND REFERENCED IN SEPARATE NOTEBOOKS. ONLY SELECTED PLOTS ARE SHOWN HERE**.\n",
    "\n",
    "The data analysis plots are in separate notebooks as they are too large to fit in one notebook. To give you an idea of the most important findings, only selected plots are shown in this notebook.\n",
    "\n",
    "We know that you asked for the explanatory notebook to contain all the code that made up this project, but we strongly believe that using our current folder structure with reference links to hyperlinks and the respective files is much more manageable and clear for the people grading this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line And Polar Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots illustrate that there are significant temporal changes across all "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre\n",
    "\n",
    "## This answer needs to be revised maybe to adapt to act 2 plots too.\n",
    "\n",
    "- *Which genre of data story did you use?*\n",
    "\n",
    "We chose the Magazine Style genre for our data story—because the New York City subway is not just a system, it is a living narrative. Events like New Year’s Eve crowd surges, the NYC Marathon, and even signal failures at Penn Station do not just show up as stats—they tell stories.\n",
    "\n",
    "Magazine Style allowed us to unfold these moments with depth and pacing. Readers can scroll at their own rhythm, pausing to explore interactive visuals or glide through snapshots of the city’s highs and lows. It is a genre designed for storytelling, reflection, and exploration—exactly how we wanted our audience to engage with the MTA’s ridership data.\n",
    "\n",
    "- *Which tools did you use from each of the 3 categories of Visual Narrative (Figure 7 in Segal and Heer). Why?*\n",
    "\n",
    "According to Segel & Heer’s Figure 7, we used tools from all three categories of Visual Narrative to build a balanced and compelling experience:\n",
    "\n",
    "  1. Annotations\n",
    "\n",
    "* We used headlines, subheadings, and descriptive figure captions to guide readers through the data.\n",
    "* For example, \"New Year’s Eve – The Ultimate Stress Test\" and \"When Protests Shut Down Stations\" are not just labels—they are entry points into the story. These annotations provided context and helped readers quickly grasp the core insight behind each chart.\n",
    "\n",
    "2. Colour and Visual Encodings\n",
    "\n",
    "* We kept a consistent colour palette across maps, line plots, and bar charts to maintain visual harmony.\n",
    "* Important data points (like surges or dips) were highlighted to stand out, ensuring quick readability even at a glance.\n",
    "\n",
    "3. Interactive Visualisations\n",
    "\n",
    "* Our Bokeh plots enabled users to explore data on their own terms. For instance, readers can hover over stations or filter by date to see how a protest in June 2020 or a snowstorm in January 2022 changed the flow.\n",
    "* This gave our story a sense of discoverability and allowed readers to become part of the analysis. <br> <br>\n",
    "\n",
    "\n",
    "\n",
    "- *Which tools did you use from each of the 3 categories of Narrative Structure (Figure 7 in Segal and Heer). Why?*\n",
    "\n",
    "We also strategically used elements from all three categories of Narrative Structure:\n",
    "\n",
    "  1. Ordering\n",
    "\n",
    "* Our story is segmented into clear acts— events, incidents, and the unseen patterns—each serving as a chapter.\n",
    "* This modular layout mirrors how magazine stories unfold, giving structure while allowing readers to jump between sections.\n",
    "\n",
    "  2. Interactivity\n",
    "\n",
    "* While much of the narrative is guided, interactive Bokeh charts give room for exploration and hypothesis testing. Want to see how 42 St ridership dropped after a crime? Click and find out.\n",
    "* This blend of narrative + interaction lets users engage more deeply than with static plots alone.\n",
    "\n",
    "  3. Messaging\n",
    "\n",
    "* Each section delivers a clear takeaway: \"New Yorkers do not stop moving,\" \"Disruptions ripple far beyond the station,\" or \"The subway mirrors the city.\"\n",
    "* These are not just data points—they are messages framed through evidence.\n",
    "\n",
    "\n",
    "Together, these visual and structural elements brought the dataset to life—not just as rows of numbers, but as a portrait of a city in motion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation\n",
    "\n",
    "- *Explain the visualizations you've chosen.*\n",
    "\n",
    "- *Why are they right for the story you want to tell?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "- *What went well?*\n",
    "\n",
    "The biggest win was our ability to translate raw ridership data into a human story. By anchoring our visuals around major New York City events—like New Year’s Eve in Times Square or signal failures at Penn Station—we made the subway system feel alive and reactive, not just mechanical.\n",
    "\n",
    "Our decision to use the Magazine Style genre turned out to be effective, as it allowed us to structure the story into individual, digestible acts-each with its own visuals, tone, and pacing-while benefiting from fewer constraints on the number of characters. This additional visual real estate allowed us to develop high-quality visualisations, most especially the interactive Bokeh plots, which successfully empowered users to explore the data for themselves by zooming in on data, filtering stations, and tracing personal narratives through the system. As a result, we were able to create a more engaging and interactive data story in our final project.\n",
    "\n",
    "- *What is still missing? What could be improved? Why?*\n",
    "\n",
    "We would have loved to include real-time video or news tweet overlays to connect ridership shifts to breaking moments as they happened—e.g., overlaying Tweets from BLM protests with station shutdowns. But pulling in those external datasets at scale (and legally) proved too time-consuming for this scope.\n",
    "\n",
    "Another challenge was granularity. While our hourly data was rich, we occasionally lacked the contextual information (e.g., exact event locations, train outages by line) that would have sharpened our causality claims. Some of our stories—like \"The Subway Avoidance Effect\"—would benefit from integrating official MTA service alerts or NYPD incident logs in future work.\n",
    "\n",
    "Lastly, accessibility could be improved: adding alt text for interactive visualisations and refining our colour scheme for better contrast would make the story more inclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contributions\n",
    "\n",
    "*You should write (just briefly) which group member was the main responsible for which elements of the assignment.* *(I want you guys to understand every part of the assignment, but usually there is someone who took lead role on certain portions of the work.* *That's what you should explain).*\n",
    "*It is not OK simply to write \"All group members contributed equally\".*\n",
    "\n",
    "Each of the 3 students has made an equal contribution to this project and each of us has helped the other and understands each component of the final outcome.\n",
    "\n",
    "However, in accordance with DTU requirements, the following is an outline of each student's main responsibilities:\n",
    "\n",
    "* **Clara Mejlhede Lorenzen (s180350)** led the data wrangling and preprocessing, including converting timestamps and structuring the data by date and hour. Clara was responsible for the GitHub Pages site, ensuring that all elements were properly linked and displayed, nonetheless, she was also in charge of implement the feedback from Assignment 2. She also contributed to the narrative structure and copy, especially in Act II (Incident reports) and the discussion section.\n",
    "\n",
    "* **Pol Triquell Lombardo (s243271)** was responsible for the narrative structure and storyboarding. He sketched the key scenes for Act I (New Year’s Eve, Marathon Sunday, protests, and incidents) and wrote most of the copy (the explainer notebook and the story), including the headlines and figure captions. Nonetheless, he also contributed on the data visualisation side, creating the interactive plots for the first Act and embedding of visuals into the GitHub Pages site.\n",
    "\n",
    "* **Marie Sophie Mudge Woods (s194384)** took charge of \n",
    "\n",
    "We met regularly to review, critique, and revise each section together, ensuring everyone understood the full scope of the story—from data wrangling to narrative arc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
